[2025-03-27T04:13:44.933+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T04:13:44.973+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T04:13:44.988+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T04:13:44.989+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 6
[2025-03-27T04:13:45.017+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-16 00:00:00+00:00
[2025-03-27T04:13:45.027+0000] {standard_task_runner.py:72} INFO - Started process 211 to run task
[2025-03-27T04:13:45.043+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-16T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmp3rrbjq9w']
[2025-03-27T04:13:45.050+0000] {standard_task_runner.py:105} INFO - Job 9: Subtask scrape_and_store
[2025-03-27T04:13:45.144+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T04:13:45.308+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-16T00:00:00+00:00'
[2025-03-27T04:13:45.310+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T04:13:45.358+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T04:14:00.079+0000] {logging_mixin.py:190} INFO - Scraping page 2...
[2025-03-27T04:14:16.285+0000] {logging_mixin.py:190} INFO - Scraping page 3...
[2025-03-27T04:14:35.458+0000] {logging_mixin.py:190} INFO - Scraping page 4...
[2025-03-27T04:14:50.191+0000] {logging_mixin.py:190} INFO - Scraping page 5...
[2025-03-27T04:15:03.760+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T04:15:03.779+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T04:15:03.810+0000] {logging_mixin.py:190} INFO - SQL dump created: /tmp/teduh_dump.sql
[2025-03-27T04:15:03.811+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 255, in run_scraper
    client = create_minio_client()
NameError: name 'create_minio_client' is not defined
[2025-03-27T04:15:03.829+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-16T00:00:00+00:00, execution_date=20250316T000000, start_date=20250327T041344, end_date=20250327T041503
[2025-03-27T04:15:03.869+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T04:15:03.871+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 9 for task scrape_and_store (name 'create_minio_client' is not defined; 211)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 255, in run_scraper
    client = create_minio_client()
NameError: name 'create_minio_client' is not defined
[2025-03-27T04:15:03.889+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T04:15:03.910+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T04:15:03.914+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T04:36:17.927+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T04:36:17.949+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T04:36:17.959+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T04:36:17.961+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 7
[2025-03-27T04:36:17.980+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-16 00:00:00+00:00
[2025-03-27T04:36:17.986+0000] {standard_task_runner.py:72} INFO - Started process 820 to run task
[2025-03-27T04:36:17.991+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-16T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmp_iaftcn4']
[2025-03-27T04:36:17.995+0000] {standard_task_runner.py:105} INFO - Job 26: Subtask scrape_and_store
[2025-03-27T04:36:18.058+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T04:36:18.526+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-16T00:00:00+00:00'
[2025-03-27T04:36:18.529+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T04:36:18.555+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T04:36:32.273+0000] {logging_mixin.py:190} INFO - Scraping page 2...
[2025-03-27T04:36:47.469+0000] {logging_mixin.py:190} INFO - Scraping page 3...
[2025-03-27T04:37:05.880+0000] {logging_mixin.py:190} INFO - Scraping page 4...
[2025-03-27T04:37:20.032+0000] {logging_mixin.py:190} INFO - Scraping page 5...
[2025-03-27T04:37:33.540+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T04:37:33.565+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T04:37:33.592+0000] {logging_mixin.py:190} INFO - SQL dump created: /tmp/teduh_dump.sql
[2025-03-27T04:37:33.595+0000] {connectionpool.py:868} WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f925f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /construction-web-scraping?location=
[2025-03-27T04:37:33.998+0000] {connectionpool.py:868} WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f93280>: Failed to establish a new connection: [Errno 111] Connection refused')': /construction-web-scraping?location=
[2025-03-27T04:37:34.801+0000] {connectionpool.py:868} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f92b30>: Failed to establish a new connection: [Errno 111] Connection refused')': /construction-web-scraping?location=
[2025-03-27T04:37:36.405+0000] {connectionpool.py:868} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f91bd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /construction-web-scraping?location=
[2025-03-27T04:37:39.610+0000] {connectionpool.py:868} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f92890>: Failed to establish a new connection: [Errno 111] Connection refused')': /construction-web-scraping?location=
[2025-03-27T04:37:39.612+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/local/lib/python3.10/http/client.py", line 1278, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.10/http/client.py", line 1038, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.10/http/client.py", line 976, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fca68f91c60>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 266, in run_scraper
    found = client.bucket_exists(MINIO_BUCKET)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 662, in bucket_exists
    self._execute("HEAD", bucket_name)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 413, in _execute
    region = self._get_region(bucket_name)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 470, in _get_region
    response = self._url_open(
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 279, in _url_open
    response = self._http.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  [Previous line repeated 2 more times]
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=9000): Max retries exceeded with url: /construction-web-scraping?location= (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f91c60>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2025-03-27T04:37:39.633+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-16T00:00:00+00:00, execution_date=20250316T000000, start_date=20250327T043617, end_date=20250327T043739
[2025-03-27T04:37:39.668+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T04:37:39.670+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 26 for task scrape_and_store (HTTPConnectionPool(host='127.0.0.1', port=9000): Max retries exceeded with url: /construction-web-scraping?location= (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f91c60>: Failed to establish a new connection: [Errno 111] Connection refused')); 820)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/local/lib/python3.10/http/client.py", line 1278, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.10/http/client.py", line 1038, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.10/http/client.py", line 976, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fca68f91c60>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 266, in run_scraper
    found = client.bucket_exists(MINIO_BUCKET)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 662, in bucket_exists
    self._execute("HEAD", bucket_name)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 413, in _execute
    region = self._get_region(bucket_name)
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 470, in _get_region
    response = self._url_open(
  File "/home/airflow/.local/lib/python3.10/site-packages/minio/api.py", line 279, in _url_open
    response = self._http.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
  [Previous line repeated 2 more times]
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=9000): Max retries exceeded with url: /construction-web-scraping?location= (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fca68f91c60>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2025-03-27T04:37:39.686+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T04:37:39.713+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T04:37:39.716+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T05:57:31.486+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T05:57:31.506+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T05:57:31.519+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T05:57:31.520+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 7
[2025-03-27T05:57:31.538+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-16 00:00:00+00:00
[2025-03-27T05:57:31.543+0000] {standard_task_runner.py:72} INFO - Started process 2783 to run task
[2025-03-27T05:57:31.548+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-16T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpgqooxned']
[2025-03-27T05:57:31.552+0000] {standard_task_runner.py:105} INFO - Job 36: Subtask scrape_and_store
[2025-03-27T05:57:31.607+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T05:57:31.711+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-16T00:00:00+00:00'
[2025-03-27T05:57:31.714+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T05:57:31.730+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T05:57:49.704+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T05:57:49.767+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T05:57:49.779+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/teduh_dump.csv
[2025-03-27T05:57:49.854+0000] {logging_mixin.py:190} INFO - Uploaded /tmp/teduh_dump.csv to minio://construction-web-scraping/teduh/teduh_dump.csv
[2025-03-27T05:57:49.855+0000] {logging_mixin.py:190} INFO - Scraping and upload done!
[2025-03-27T05:57:49.858+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-27T05:57:49.876+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T05:57:49.883+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-16T00:00:00+00:00, execution_date=20250316T000000, start_date=20250327T055731, end_date=20250327T055749
[2025-03-27T05:57:49.976+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-27T05:57:50.010+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T05:57:50.016+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:15:28.193+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:15:28.218+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T06:15:28.229+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T06:15:28.232+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 7
[2025-03-27T06:15:28.253+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-16 00:00:00+00:00
[2025-03-27T06:15:28.258+0000] {standard_task_runner.py:72} INFO - Started process 3311 to run task
[2025-03-27T06:15:28.263+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-16T00:00:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpsdew3vrn']
[2025-03-27T06:15:28.267+0000] {standard_task_runner.py:105} INFO - Job 48: Subtask scrape_and_store
[2025-03-27T06:15:28.355+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:15:28.475+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-16T00:00:00+00:00'
[2025-03-27T06:15:28.477+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:15:28.495+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:15:44.522+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T06:15:44.555+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T06:15:44.559+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/construction_20250327.csv
[2025-03-27T06:15:44.579+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 257, in run_scraper
    object_name=MINIO_OBJECT,
NameError: name 'MINIO_OBJECT' is not defined
[2025-03-27T06:15:44.596+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-16T00:00:00+00:00, execution_date=20250316T000000, start_date=20250327T061528, end_date=20250327T061544
[2025-03-27T06:15:44.649+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:15:44.651+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 48 for task scrape_and_store (name 'MINIO_OBJECT' is not defined; 3311)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 257, in run_scraper
    object_name=MINIO_OBJECT,
NameError: name 'MINIO_OBJECT' is not defined
[2025-03-27T06:15:44.708+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T06:15:44.737+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T06:15:44.740+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:40:55.743+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:40:55.768+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T06:40:55.780+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [queued]>
[2025-03-27T06:40:55.782+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 6
[2025-03-27T06:40:55.806+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-16 00:00:00+00:00
[2025-03-27T06:40:55.813+0000] {standard_task_runner.py:72} INFO - Started process 4159 to run task
[2025-03-27T06:40:55.817+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-16T00:00:00+00:00', '--job-id', '62', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpm8grf96y']
[2025-03-27T06:40:55.821+0000] {standard_task_runner.py:105} INFO - Job 62: Subtask scrape_and_store
[2025-03-27T06:40:55.885+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-16T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:40:55.978+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-16T00:00:00+00:00'
[2025-03-27T06:40:55.980+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:40:55.997+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:41:12.176+0000] {logging_mixin.py:190} INFO - Scraping page 2...
[2025-03-27T06:41:33.410+0000] {logging_mixin.py:190} INFO - Scraping page 3...
[2025-03-27T06:41:54.091+0000] {logging_mixin.py:190} INFO - Scraping page 4...
[2025-03-27T06:42:11.473+0000] {logging_mixin.py:190} INFO - Scraping page 5...
[2025-03-27T06:42:27.195+0000] {logging_mixin.py:190} INFO - Scraping page 6...
[2025-03-27T06:42:45.175+0000] {logging_mixin.py:190} INFO - Scraping page 7...
[2025-03-27T06:43:04.409+0000] {logging_mixin.py:190} INFO - Scraping page 8...
[2025-03-27T06:43:20.553+0000] {logging_mixin.py:190} INFO - Scraping page 9...
[2025-03-27T06:43:42.701+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30590-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30590-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca682c0cd0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:43:46.161+0000] {logging_mixin.py:190} INFO - Scraping page 10...
[2025-03-27T06:44:03.689+0000] {logging_mixin.py:190} INFO - Scraping page 11...
[2025-03-27T06:44:21.076+0000] {logging_mixin.py:190} INFO - Scraping page 12...
[2025-03-27T06:44:38.264+0000] {logging_mixin.py:190} INFO - Scraping page 13...
[2025-03-27T06:44:55.841+0000] {logging_mixin.py:190} INFO - Scraping page 14...
[2025-03-27T06:45:11.959+0000] {logging_mixin.py:190} INFO - Scraping page 15...
[2025-03-27T06:45:29.301+0000] {logging_mixin.py:190} INFO - Scraping page 16...
[2025-03-27T06:45:46.379+0000] {logging_mixin.py:190} INFO - Scraping page 17...
[2025-03-27T06:46:04.372+0000] {logging_mixin.py:190} INFO - Scraping page 18...
[2025-03-27T06:46:23.353+0000] {logging_mixin.py:190} INFO - Scraping page 19...
[2025-03-27T06:46:38.932+0000] {logging_mixin.py:190} INFO - Scraping page 20...
[2025-03-27T06:46:59.353+0000] {logging_mixin.py:190} INFO - Scraping page 21...
[2025-03-27T06:47:20.378+0000] {logging_mixin.py:190} INFO - Scraping page 22...
[2025-03-27T06:47:43.149+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30263-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30263-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68422350>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:47:48.513+0000] {logging_mixin.py:190} INFO - Scraping page 23...
[2025-03-27T06:48:04.509+0000] {logging_mixin.py:190} INFO - Scraping page 24...
[2025-03-27T06:48:21.290+0000] {logging_mixin.py:190} INFO - Scraping page 25...
[2025-03-27T06:48:38.294+0000] {logging_mixin.py:190} INFO - Scraping page 26...
[2025-03-27T06:48:56.432+0000] {logging_mixin.py:190} INFO - Scraping page 27...
[2025-03-27T06:49:22.332+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30156-5: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30156-5 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68952380>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:49:24.963+0000] {logging_mixin.py:190} INFO - Scraping page 28...
[2025-03-27T06:49:41.737+0000] {logging_mixin.py:190} INFO - Scraping page 29...
[2025-03-27T06:49:58.964+0000] {logging_mixin.py:190} INFO - Scraping page 30...
[2025-03-27T06:50:19.058+0000] {logging_mixin.py:190} INFO - Scraping page 31...
[2025-03-27T06:50:36.519+0000] {logging_mixin.py:190} INFO - Scraping page 32...
[2025-03-27T06:50:57.529+0000] {logging_mixin.py:190} INFO - Scraping page 33...
[2025-03-27T06:51:18.425+0000] {logging_mixin.py:190} INFO - Scraping page 34...
[2025-03-27T06:51:37.633+0000] {logging_mixin.py:190} INFO - Scraping page 35...
[2025-03-27T06:51:54.108+0000] {logging_mixin.py:190} INFO - Scraping page 36...
[2025-03-27T06:52:14.058+0000] {logging_mixin.py:190} INFO - Scraping page 37...
[2025-03-27T06:52:31.974+0000] {logging_mixin.py:190} INFO - Scraping page 38...
[2025-03-27T06:52:52.106+0000] {logging_mixin.py:190} INFO - Scraping page 39...
[2025-03-27T06:53:11.056+0000] {logging_mixin.py:190} INFO - Scraping page 40...
[2025-03-27T06:53:28.602+0000] {logging_mixin.py:190} INFO - Scraping page 41...
[2025-03-27T06:53:46.195+0000] {logging_mixin.py:190} INFO - Scraping page 42...
[2025-03-27T06:54:04.227+0000] {logging_mixin.py:190} INFO - Scraping page 43...
[2025-03-27T06:54:20.764+0000] {logging_mixin.py:190} INFO - Scraping page 44...
[2025-03-27T06:54:37.533+0000] {logging_mixin.py:190} INFO - Scraping page 45...
[2025-03-27T06:54:56.881+0000] {logging_mixin.py:190} INFO - Scraping page 46...
[2025-03-27T06:55:13.107+0000] {logging_mixin.py:190} INFO - Scraping page 47...
[2025-03-27T06:55:31.083+0000] {logging_mixin.py:190} INFO - Scraping page 48...
[2025-03-27T06:55:47.682+0000] {logging_mixin.py:190} INFO - Scraping page 49...
[2025-03-27T06:56:05.474+0000] {logging_mixin.py:190} INFO - Scraping page 50...
[2025-03-27T06:56:23.946+0000] {logging_mixin.py:190} INFO - Scraping page 51...
[2025-03-27T06:56:41.664+0000] {logging_mixin.py:190} INFO - Scraping page 52...
[2025-03-27T06:56:59.609+0000] {logging_mixin.py:190} INFO - Scraping page 53...
[2025-03-27T06:57:16.141+0000] {logging_mixin.py:190} INFO - Scraping page 54...
[2025-03-27T06:57:33.165+0000] {logging_mixin.py:190} INFO - Scraping page 55...
[2025-03-27T06:57:50.929+0000] {logging_mixin.py:190} INFO - Scraping page 56...
[2025-03-27T06:58:07.825+0000] {logging_mixin.py:190} INFO - Scraping page 57...
[2025-03-27T06:58:26.171+0000] {logging_mixin.py:190} INFO - Scraping page 58...
[2025-03-27T06:58:43.364+0000] {logging_mixin.py:190} INFO - Scraping page 59...
[2025-03-27T06:58:59.725+0000] {logging_mixin.py:190} INFO - Scraping page 60...
[2025-03-27T06:59:22.034+0000] {logging_mixin.py:190} INFO - Scraping page 61...
[2025-03-27T06:59:38.690+0000] {logging_mixin.py:190} INFO - Scraping page 62...
[2025-03-27T06:59:56.261+0000] {logging_mixin.py:190} INFO - Scraping page 63...
[2025-03-27T07:00:19.128+0000] {logging_mixin.py:190} INFO - Scraping page 64...
[2025-03-27T07:00:36.726+0000] {logging_mixin.py:190} INFO - Scraping page 65...
[2025-03-27T07:00:53.300+0000] {logging_mixin.py:190} INFO - Scraping page 66...
[2025-03-27T07:01:14.886+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19794-16: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19794-16 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68618b80>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:01:18.694+0000] {logging_mixin.py:190} INFO - Scraping page 67...
[2025-03-27T07:01:34.672+0000] {logging_mixin.py:190} INFO - Scraping page 68...
[2025-03-27T07:01:54.532+0000] {logging_mixin.py:190} INFO - Scraping page 69...
[2025-03-27T07:02:12.249+0000] {logging_mixin.py:190} INFO - Scraping page 70...
[2025-03-27T07:02:29.062+0000] {logging_mixin.py:190} INFO - Scraping page 71...
[2025-03-27T07:02:48.517+0000] {logging_mixin.py:190} INFO - Scraping page 72...
[2025-03-27T07:03:07.045+0000] {logging_mixin.py:190} INFO - Scraping page 73...
[2025-03-27T07:03:25.235+0000] {logging_mixin.py:190} INFO - Scraping page 74...
[2025-03-27T07:03:44.912+0000] {logging_mixin.py:190} INFO - Scraping page 75...
[2025-03-27T07:04:01.584+0000] {logging_mixin.py:190} INFO - Scraping page 76...
[2025-03-27T07:04:20.035+0000] {logging_mixin.py:190} INFO - Scraping page 77...
[2025-03-27T07:04:37.731+0000] {logging_mixin.py:190} INFO - Scraping page 78...
[2025-03-27T07:05:01.441+0000] {logging_mixin.py:190} INFO - Scraping page 79...
[2025-03-27T07:05:20.684+0000] {logging_mixin.py:190} INFO - Scraping page 80...
[2025-03-27T07:05:44.552+0000] {logging_mixin.py:190} INFO - Scraping page 81...
[2025-03-27T07:06:01.726+0000] {logging_mixin.py:190} INFO - Scraping page 82...
[2025-03-27T07:06:21.809+0000] {logging_mixin.py:190} INFO - Scraping page 83...
[2025-03-27T07:06:40.587+0000] {logging_mixin.py:190} INFO - Scraping page 84...
[2025-03-27T07:06:57.967+0000] {logging_mixin.py:190} INFO - Scraping page 85...
[2025-03-27T07:07:15.252+0000] {logging_mixin.py:190} INFO - Scraping page 86...
[2025-03-27T07:07:34.947+0000] {logging_mixin.py:190} INFO - Scraping page 87...
[2025-03-27T07:07:51.422+0000] {logging_mixin.py:190} INFO - Scraping page 88...
[2025-03-27T07:08:12.134+0000] {logging_mixin.py:190} INFO - Scraping page 89...
[2025-03-27T07:08:29.842+0000] {logging_mixin.py:190} INFO - Scraping page 90...
[2025-03-27T07:08:45.784+0000] {logging_mixin.py:190} INFO - Scraping page 91...
[2025-03-27T07:09:02.564+0000] {logging_mixin.py:190} INFO - Scraping page 92...
[2025-03-27T07:09:20.638+0000] {logging_mixin.py:190} INFO - Scraping page 93...
[2025-03-27T07:09:36.161+0000] {logging_mixin.py:190} INFO - Scraping page 94...
[2025-03-27T07:09:53.060+0000] {logging_mixin.py:190} INFO - Scraping page 95...
[2025-03-27T07:10:12.085+0000] {logging_mixin.py:190} INFO - Scraping page 96...
[2025-03-27T07:10:30.504+0000] {logging_mixin.py:190} INFO - Scraping page 97...
[2025-03-27T07:10:46.060+0000] {logging_mixin.py:190} INFO - Scraping page 98...
[2025-03-27T07:10:55.884+0000] {timeout.py:68} ERROR - Process timed out, PID: 4159
[2025-03-27T07:10:55.901+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 763, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 183, in run_scraper
    daerah, negeri, harga_min, harga_maks = scrape_additional_data(link)
  File "/opt/airflow/dags/construction_web_scraping.py", line 45, in scrape_additional_data
    response = requests.get(link, timeout=10)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
  File "/usr/local/lib/python3.10/http/client.py", line 1375, in getresponse
    response.begin()
  File "/usr/local/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/usr/local/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/local/lib/python3.10/socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/lib/python3.10/ssl.py", line 1307, in recv_into
    return self.read(nbytes, buffer)
  File "/usr/local/lib/python3.10/ssl.py", line 1163, in read
    return self._sslobj.read(len, buffer)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: Timeout, PID: 4159
[2025-03-27T07:10:55.919+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-16T00:00:00+00:00, execution_date=20250316T000000, start_date=20250327T064055, end_date=20250327T071055
[2025-03-27T07:10:55.967+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T07:10:56.006+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 2
[2025-03-27T07:10:56.038+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T07:10:56.041+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
