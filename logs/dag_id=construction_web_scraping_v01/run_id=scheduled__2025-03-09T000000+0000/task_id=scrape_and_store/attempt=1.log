[2025-03-27T03:53:37.944+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T03:53:37.991+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T03:53:38.013+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T03:53:38.018+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T03:53:38.090+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T03:53:38.099+0000] {standard_task_runner.py:72} INFO - Started process 1383 to run task
[2025-03-27T03:53:38.106+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmplmsw8lfg']
[2025-03-27T03:53:38.117+0000] {standard_task_runner.py:105} INFO - Job 5: Subtask scrape_and_store
[2025-03-27T03:53:38.225+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host 7852a38dca40
[2025-03-27T03:53:38.348+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T03:53:38.351+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T03:53:38.376+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T03:53:38.378+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 138, in run_scraper
    current_headers, rows = scrape_page(page_number)
  File "/opt/airflow/dags/construction_web_scraping.py", line 99, in scrape_page
    response = requests.get(url)
NameError: name 'requests' is not defined
[2025-03-27T03:53:38.402+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T035337, end_date=20250327T035338
[2025-03-27T03:53:38.442+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T03:53:38.443+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 5 for task scrape_and_store (name 'requests' is not defined; 1383)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 138, in run_scraper
    current_headers, rows = scrape_page(page_number)
  File "/opt/airflow/dags/construction_web_scraping.py", line 99, in scrape_page
    response = requests.get(url)
NameError: name 'requests' is not defined
[2025-03-27T03:53:38.484+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T03:53:38.517+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T03:53:38.519+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T04:32:06.824+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T04:32:06.861+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T04:32:06.882+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T04:32:06.885+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T04:32:06.917+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T04:32:06.929+0000] {standard_task_runner.py:72} INFO - Started process 696 to run task
[2025-03-27T04:32:06.935+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpuw8rslkd']
[2025-03-27T04:32:06.945+0000] {standard_task_runner.py:105} INFO - Job 21: Subtask scrape_and_store
[2025-03-27T04:32:07.074+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T04:32:07.343+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T04:32:07.348+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T04:32:07.374+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T04:32:23.963+0000] {logging_mixin.py:190} INFO - Scraping page 2...
[2025-03-27T04:32:41.421+0000] {logging_mixin.py:190} INFO - Scraping page 3...
[2025-03-27T04:33:01.665+0000] {logging_mixin.py:190} INFO - Scraping page 4...
[2025-03-27T04:33:16.222+0000] {logging_mixin.py:190} INFO - Scraping page 5...
[2025-03-27T04:33:30.717+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T04:33:30.737+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T04:33:30.766+0000] {logging_mixin.py:190} INFO - SQL dump created: /tmp/teduh_dump.sql
[2025-03-27T04:33:30.767+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 255, in run_scraper
    client = create_minio_client()
NameError: name 'create_minio_client' is not defined
[2025-03-27T04:33:30.784+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T043206, end_date=20250327T043330
[2025-03-27T04:33:30.818+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T04:33:30.819+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 21 for task scrape_and_store (name 'create_minio_client' is not defined; 696)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 255, in run_scraper
    client = create_minio_client()
NameError: name 'create_minio_client' is not defined
[2025-03-27T04:33:30.862+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T04:33:30.887+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T04:33:30.891+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T05:56:31.516+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T05:56:31.544+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T05:56:31.558+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T05:56:31.559+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T05:56:31.584+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T05:56:31.591+0000] {standard_task_runner.py:72} INFO - Started process 2751 to run task
[2025-03-27T05:56:31.596+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpb5rsux0t']
[2025-03-27T05:56:31.601+0000] {standard_task_runner.py:105} INFO - Job 32: Subtask scrape_and_store
[2025-03-27T05:56:31.678+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T05:56:31.810+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T05:56:31.813+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T05:56:31.833+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T05:56:50.815+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T05:56:50.858+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T05:56:50.872+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/teduh_dump.csv
[2025-03-27T05:56:50.931+0000] {logging_mixin.py:190} INFO - Uploaded /tmp/teduh_dump.csv to minio://construction-web-scraping/teduh/teduh_dump.csv
[2025-03-27T05:56:50.932+0000] {logging_mixin.py:190} INFO - Scraping and upload done!
[2025-03-27T05:56:50.935+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-27T05:56:50.971+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T05:56:50.974+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T055631, end_date=20250327T055650
[2025-03-27T05:56:51.088+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-27T05:56:51.111+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T05:56:51.115+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:08:14.494+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:08:14.519+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:08:14.531+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:08:14.533+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:08:14.553+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:08:14.560+0000] {standard_task_runner.py:72} INFO - Started process 3077 to run task
[2025-03-27T06:08:14.565+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmphb5sfypl']
[2025-03-27T06:08:14.569+0000] {standard_task_runner.py:105} INFO - Job 42: Subtask scrape_and_store
[2025-03-27T06:08:14.632+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:08:14.733+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:08:14.735+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:08:14.754+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:08:30.588+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T06:08:30.613+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T06:08:30.617+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/construction_20250327.csv
[2025-03-27T06:08:30.670+0000] {logging_mixin.py:190} INFO - Uploaded /tmp/construction_20250327.csv to minio://construction-web-scraping/teduh/construction_{today_str}.csv
[2025-03-27T06:08:30.672+0000] {logging_mixin.py:190} INFO - Scraping and upload done!
[2025-03-27T06:08:30.674+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-27T06:08:30.687+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:08:30.689+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T060814, end_date=20250327T060830
[2025-03-27T06:08:30.748+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-27T06:08:30.764+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T06:08:30.767+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:13:43.348+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:13:43.374+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:13:43.390+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:13:43.391+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:13:43.411+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:13:43.420+0000] {standard_task_runner.py:72} INFO - Started process 3243 to run task
[2025-03-27T06:13:43.425+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '44', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmp2unsyovq']
[2025-03-27T06:13:43.429+0000] {standard_task_runner.py:105} INFO - Job 44: Subtask scrape_and_store
[2025-03-27T06:13:43.512+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:13:43.682+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:13:43.684+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:13:43.729+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:14:00.313+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T06:14:00.337+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T06:14:00.341+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/construction_20250327.csv
[2025-03-27T06:14:00.358+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 257, in run_scraper
    object_name=MINIO_CSV_OBJECT,
NameError: name 'MINIO_CSV_OBJECT' is not defined
[2025-03-27T06:14:00.377+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T061343, end_date=20250327T061400
[2025-03-27T06:14:00.412+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:14:00.414+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 44 for task scrape_and_store (name 'MINIO_CSV_OBJECT' is not defined; 3243)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 257, in run_scraper
    object_name=MINIO_CSV_OBJECT,
NameError: name 'MINIO_CSV_OBJECT' is not defined
[2025-03-27T06:14:00.464+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T06:14:00.487+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T06:14:00.490+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:21:29.630+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:21:29.666+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:21:29.679+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:21:29.680+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:21:29.702+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:21:29.707+0000] {standard_task_runner.py:72} INFO - Started process 3573 to run task
[2025-03-27T06:21:29.711+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpwcjjlv_3']
[2025-03-27T06:21:29.714+0000] {standard_task_runner.py:105} INFO - Job 50: Subtask scrape_and_store
[2025-03-27T06:21:29.770+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:21:29.877+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:21:29.879+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:21:29.899+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:21:47.393+0000] {logging_mixin.py:190} INFO - Columns before rename: ['BIL.', 'KOD PROJEK', 'PEMAJU', 'PROJEK', 'NO. PERMIT', 'STATUS PROJEK KESELURUHAN', 'RINGKASAN PROJEK', 'Daerah Projek', 'Negeri Projek', 'Harga Minimum (RM)', 'Harga Maksimum (RM)']
[2025-03-27T06:21:47.425+0000] {logging_mixin.py:190} INFO -   Bil  ...                                  RINGKASAN PROJEK
0   1  ...  https://teduh.kpkt.gov.my/project-swasta/30969-1
1   2  ...  https://teduh.kpkt.gov.my/project-swasta/30946-1
2   3  ...  https://teduh.kpkt.gov.my/project-swasta/30937-1
3   4  ...  https://teduh.kpkt.gov.my/project-swasta/30921-1
4   5  ...  https://teduh.kpkt.gov.my/project-swasta/30920-1
5   6  ...  https://teduh.kpkt.gov.my/project-swasta/30914-1
6   7  ...  https://teduh.kpkt.gov.my/project-swasta/30913-1
7   8  ...  https://teduh.kpkt.gov.my/project-swasta/30909-1
8   9  ...  https://teduh.kpkt.gov.my/project-swasta/30908-1
9  10  ...  https://teduh.kpkt.gov.my/project-swasta/30902-1

[10 rows x 11 columns]
[2025-03-27T06:21:47.431+0000] {logging_mixin.py:190} INFO - CSV file created: /tmp/construction_20250327.csv
[2025-03-27T06:21:47.493+0000] {logging_mixin.py:190} INFO - Uploaded /tmp/construction_20250327.csv to minio://construction-web-scraping/teduh/construction_20250327.csv
[2025-03-27T06:21:47.494+0000] {logging_mixin.py:190} INFO - Scraping and upload done!
[2025-03-27T06:21:47.496+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-27T06:21:47.511+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:21:47.513+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T062129, end_date=20250327T062147
[2025-03-27T06:21:47.595+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-27T06:21:47.611+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T06:21:47.615+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:24:32.831+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:24:32.853+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:24:32.869+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:24:32.871+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:24:32.893+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:24:32.900+0000] {standard_task_runner.py:72} INFO - Started process 3680 to run task
[2025-03-27T06:24:32.908+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '53', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpzh4zexoj']
[2025-03-27T06:24:32.913+0000] {standard_task_runner.py:105} INFO - Job 53: Subtask scrape_and_store
[2025-03-27T06:24:33.004+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:24:33.162+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:24:33.164+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:24:33.186+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 170, in run_scraper
    print(f"Scraping page {page_number}...")
NameError: name 'page_number' is not defined
[2025-03-27T06:24:33.202+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T062432, end_date=20250327T062433
[2025-03-27T06:24:33.237+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:24:33.238+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 53 for task scrape_and_store (name 'page_number' is not defined; 3680)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 170, in run_scraper
    print(f"Scraping page {page_number}...")
NameError: name 'page_number' is not defined
[2025-03-27T06:24:33.283+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-03-27T06:24:33.307+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-27T06:24:33.310+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:27:46.663+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:27:46.687+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:27:46.701+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:27:46.702+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:27:46.724+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:27:46.730+0000] {standard_task_runner.py:72} INFO - Started process 3767 to run task
[2025-03-27T06:27:46.736+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '55', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmploi395x_']
[2025-03-27T06:27:46.739+0000] {standard_task_runner.py:105} INFO - Job 55: Subtask scrape_and_store
[2025-03-27T06:27:46.806+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:27:46.905+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:27:46.907+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:27:46.924+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:28:07.671+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:28:24.723+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:28:41.335+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:29:00.526+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30899-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30899-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68a762f0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:29:13.957+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30875-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30875-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca687b3550>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:29:17.621+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:29:36.348+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:29:53.099+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:30:09.393+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:30:24.811+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:30:38.659+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30946-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30946-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68d6e590>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:30:50.529+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:31:07.628+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:31:28.713+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30882-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30882-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca686096f0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:31:32.694+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:31:52.845+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:32:10.438+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:32:29.169+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:32:46.921+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:32:50.385+0000] {local_task_job_runner.py:346} WARNING - State of this instance has been externally set to restarting. Terminating instance.
[2025-03-27T06:32:50.387+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T06:32:50.389+0000] {process_utils.py:132} INFO - Sending 15 to group 3767. PIDs of all processes in the group: [3767]
[2025-03-27T06:32:50.391+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 3767
[2025-03-27T06:32:50.394+0000] {taskinstance.py:3094} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-03-27T06:32:50.404+0000] {taskinstance.py:3095} ERROR - Stacktrace: 
  File "/home/***/.local/bin/***", line 8, in <module>
    sys.exit(main())
  File "/home/***/.local/lib/python3.10/site-packages/***/__main__.py", line 62, in main
    args.func(args)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 56, in scheduler
    run_command_with_daemon_option(
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 59, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/scheduler_job_runner.py", line 990, in _execute
    executor.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 392, in start
    self.impl.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 330, in start
    worker.start()
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 80, in run
    return super().run()
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 211, in do_work
    self.execute_work(key=key, command=command)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 100, in execute_work
    state = self._execute_work_in_fork(command)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 142, in _execute_work_in_fork
    args.func(args)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 254, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 322, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/local_task_job_runner.py", line 171, in _execute
    self.task_runner.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/task/task_runner/standard_task_runner.py", line 55, in start
    self.process = self._start_by_fork()
  File "/home/***/.local/lib/python3.10/site-packages/***/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/***/dags/construction_web_scraping.py", line 183, in run_scraper
    daerah, negeri, harga_min, harga_maks = scrape_additional_data(link)
  File "/opt/***/dags/construction_web_scraping.py", line 45, in scrape_additional_data
    response = requests.get(link, timeout=10)
  File "/home/***/.local/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connection.py", line 741, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connection.py", line 920, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py", line 460, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py", line 504, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/local/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/usr/local/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/usr/local/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3095, in signal_handler
    self.log.error("Stacktrace: \n%s", "".join(traceback.format_stack()))

[2025-03-27T06:32:50.419+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 183, in run_scraper
    daerah, negeri, harga_min, harga_maks = scrape_additional_data(link)
  File "/opt/airflow/dags/construction_web_scraping.py", line 45, in scrape_additional_data
    response = requests.get(link, timeout=10)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 741, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 920, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py", line 460, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py", line 504, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/local/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/usr/local/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/usr/local/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3097, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2025-03-27T06:32:50.424+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T062746, end_date=20250327T063250
[2025-03-27T06:32:50.456+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T06:32:50.487+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=3767, status='terminated', exitcode=2, started='06:27:46') (3767) terminated with exit code 2
[2025-03-27T06:33:11.948+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T06:33:11.976+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:33:11.988+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [queued]>
[2025-03-27T06:33:11.989+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 6
[2025-03-27T06:33:12.008+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): scrape_and_store> on 2025-03-09 00:00:00+00:00
[2025-03-27T06:33:12.014+0000] {standard_task_runner.py:72} INFO - Started process 3945 to run task
[2025-03-27T06:33:12.019+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'construction_web_scraping_v01', 'scrape_and_store', 'scheduled__2025-03-09T00:00:00+00:00', '--job-id', '58', '--raw', '--subdir', 'DAGS_FOLDER/construction_web_scraping.py', '--cfg-path', '/tmp/tmpc392zbc2']
[2025-03-27T06:33:12.023+0000] {standard_task_runner.py:105} INFO - Job 58: Subtask scrape_and_store
[2025-03-27T06:33:12.078+0000] {task_command.py:467} INFO - Running <TaskInstance: construction_web_scraping_v01.scrape_and_store scheduled__2025-03-09T00:00:00+00:00 [running]> on host c79bd584e644
[2025-03-27T06:33:12.177+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='waiyong' AIRFLOW_CTX_DAG_ID='construction_web_scraping_v01' AIRFLOW_CTX_TASK_ID='scrape_and_store' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-09T00:00:00+00:00'
[2025-03-27T06:33:12.178+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T06:33:12.194+0000] {logging_mixin.py:190} INFO - Scraping page 1...
[2025-03-27T06:33:31.112+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30886-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30886-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68ceb0d0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:33:37.468+0000] {logging_mixin.py:190} INFO - Scraping page 2...
[2025-03-27T06:33:54.612+0000] {logging_mixin.py:190} INFO - Scraping page 3...
[2025-03-27T06:34:15.074+0000] {logging_mixin.py:190} INFO - Scraping page 4...
[2025-03-27T06:34:31.869+0000] {logging_mixin.py:190} INFO - Scraping page 5...
[2025-03-27T06:34:49.365+0000] {logging_mixin.py:190} INFO - Scraping page 6...
[2025-03-27T06:35:07.160+0000] {logging_mixin.py:190} INFO - Scraping page 7...
[2025-03-27T06:35:24.004+0000] {logging_mixin.py:190} INFO - Scraping page 8...
[2025-03-27T06:35:41.049+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30635-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30635-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68628a30>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:35:51.104+0000] {logging_mixin.py:190} INFO - Scraping page 9...
[2025-03-27T06:36:09.560+0000] {logging_mixin.py:190} INFO - Scraping page 10...
[2025-03-27T06:36:27.256+0000] {logging_mixin.py:190} INFO - Scraping page 11...
[2025-03-27T06:36:43.535+0000] {logging_mixin.py:190} INFO - Scraping page 12...
[2025-03-27T06:37:02.703+0000] {logging_mixin.py:190} INFO - Scraping page 13...
[2025-03-27T06:37:23.013+0000] {logging_mixin.py:190} INFO - Scraping page 14...
[2025-03-27T06:37:40.372+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30470-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30470-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68a63970>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:37:49.863+0000] {logging_mixin.py:190} INFO - Scraping page 15...
[2025-03-27T06:38:12.972+0000] {logging_mixin.py:190} INFO - Scraping page 16...
[2025-03-27T06:38:32.093+0000] {logging_mixin.py:190} INFO - Scraping page 17...
[2025-03-27T06:38:53.427+0000] {logging_mixin.py:190} INFO - Scraping page 18...
[2025-03-27T06:39:14.573+0000] {logging_mixin.py:190} INFO - Scraping page 19...
[2025-03-27T06:39:33.046+0000] {logging_mixin.py:190} INFO - Scraping page 20...
[2025-03-27T06:39:50.232+0000] {logging_mixin.py:190} INFO - Scraping page 21...
[2025-03-27T06:40:06.902+0000] {logging_mixin.py:190} INFO - Scraping page 22...
[2025-03-27T06:40:24.097+0000] {logging_mixin.py:190} INFO - Scraping page 23...
[2025-03-27T06:40:42.740+0000] {logging_mixin.py:190} INFO - Scraping page 24...
[2025-03-27T06:40:59.431+0000] {logging_mixin.py:190} INFO - Scraping page 25...
[2025-03-27T06:41:17.065+0000] {logging_mixin.py:190} INFO - Scraping page 26...
[2025-03-27T06:41:42.053+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/30174-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/30174-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68e58eb0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:41:46.058+0000] {logging_mixin.py:190} INFO - Scraping page 27...
[2025-03-27T06:42:04.530+0000] {logging_mixin.py:190} INFO - Scraping page 28...
[2025-03-27T06:42:24.731+0000] {logging_mixin.py:190} INFO - Scraping page 29...
[2025-03-27T06:42:43.094+0000] {logging_mixin.py:190} INFO - Scraping page 30...
[2025-03-27T06:43:01.700+0000] {logging_mixin.py:190} INFO - Scraping page 31...
[2025-03-27T06:43:17.308+0000] {logging_mixin.py:190} INFO - Scraping page 32...
[2025-03-27T06:43:33.939+0000] {logging_mixin.py:190} INFO - Scraping page 33...
[2025-03-27T06:43:54.159+0000] {logging_mixin.py:190} INFO - Scraping page 34...
[2025-03-27T06:44:15.506+0000] {logging_mixin.py:190} INFO - Scraping page 35...
[2025-03-27T06:44:32.834+0000] {logging_mixin.py:190} INFO - Scraping page 36...
[2025-03-27T06:44:51.240+0000] {logging_mixin.py:190} INFO - Scraping page 37...
[2025-03-27T06:45:10.396+0000] {logging_mixin.py:190} INFO - Scraping page 38...
[2025-03-27T06:45:27.892+0000] {logging_mixin.py:190} INFO - Scraping page 39...
[2025-03-27T06:45:45.733+0000] {logging_mixin.py:190} INFO - Scraping page 40...
[2025-03-27T06:46:08.396+0000] {logging_mixin.py:190} INFO - Scraping page 41...
[2025-03-27T06:46:26.821+0000] {logging_mixin.py:190} INFO - Scraping page 42...
[2025-03-27T06:46:45.177+0000] {logging_mixin.py:190} INFO - Scraping page 43...
[2025-03-27T06:47:05.786+0000] {logging_mixin.py:190} INFO - Scraping page 44...
[2025-03-27T06:47:25.056+0000] {logging_mixin.py:190} INFO - Scraping page 45...
[2025-03-27T06:47:42.933+0000] {logging_mixin.py:190} INFO - Scraping page 46...
[2025-03-27T06:48:01.747+0000] {logging_mixin.py:190} INFO - Scraping page 47...
[2025-03-27T06:48:19.277+0000] {logging_mixin.py:190} INFO - Scraping page 48...
[2025-03-27T06:48:38.245+0000] {logging_mixin.py:190} INFO - Scraping page 49...
[2025-03-27T06:48:58.039+0000] {logging_mixin.py:190} INFO - Scraping page 50...
[2025-03-27T06:49:16.133+0000] {logging_mixin.py:190} INFO - Scraping page 51...
[2025-03-27T06:49:35.788+0000] {logging_mixin.py:190} INFO - Scraping page 52...
[2025-03-27T06:49:56.881+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/20005-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/20005-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68e5a500>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:50:04.510+0000] {logging_mixin.py:190} INFO - Scraping page 53...
[2025-03-27T06:50:25.293+0000] {logging_mixin.py:190} INFO - Scraping page 54...
[2025-03-27T06:50:47.065+0000] {logging_mixin.py:190} INFO - Scraping page 55...
[2025-03-27T06:51:06.540+0000] {logging_mixin.py:190} INFO - Scraping page 56...
[2025-03-27T06:51:25.728+0000] {logging_mixin.py:190} INFO - Scraping page 57...
[2025-03-27T06:51:45.822+0000] {logging_mixin.py:190} INFO - Scraping page 58...
[2025-03-27T06:52:03.087+0000] {logging_mixin.py:190} INFO - Scraping page 59...
[2025-03-27T06:52:21.396+0000] {logging_mixin.py:190} INFO - Scraping page 60...
[2025-03-27T06:52:40.822+0000] {logging_mixin.py:190} INFO - Scraping page 61...
[2025-03-27T06:53:00.023+0000] {logging_mixin.py:190} INFO - Scraping page 62...
[2025-03-27T06:53:18.441+0000] {logging_mixin.py:190} INFO - Scraping page 63...
[2025-03-27T06:53:39.895+0000] {logging_mixin.py:190} INFO - Scraping page 64...
[2025-03-27T06:53:58.438+0000] {logging_mixin.py:190} INFO - Scraping page 65...
[2025-03-27T06:54:15.743+0000] {logging_mixin.py:190} INFO - Scraping page 66...
[2025-03-27T06:54:35.395+0000] {logging_mixin.py:190} INFO - Scraping page 67...
[2025-03-27T06:54:54.096+0000] {logging_mixin.py:190} INFO - Scraping page 68...
[2025-03-27T06:55:12.440+0000] {logging_mixin.py:190} INFO - Scraping page 69...
[2025-03-27T06:55:32.376+0000] {logging_mixin.py:190} INFO - Scraping page 70...
[2025-03-27T06:55:48.740+0000] {logging_mixin.py:190} INFO - Scraping page 71...
[2025-03-27T06:56:09.634+0000] {logging_mixin.py:190} INFO - Scraping page 72...
[2025-03-27T06:56:26.460+0000] {logging_mixin.py:190} INFO - Scraping page 73...
[2025-03-27T06:56:50.648+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19713-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19713-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68820310>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:56:53.517+0000] {logging_mixin.py:190} INFO - Scraping page 74...
[2025-03-27T06:57:11.260+0000] {logging_mixin.py:190} INFO - Scraping page 75...
[2025-03-27T06:57:27.058+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19696-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19696-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68a60700>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:57:37.468+0000] {logging_mixin.py:190} INFO - Scraping page 76...
[2025-03-27T06:57:53.828+0000] {logging_mixin.py:190} INFO - Scraping page 77...
[2025-03-27T06:58:18.085+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19654-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19654-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68b8ef80>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:58:21.461+0000] {logging_mixin.py:190} INFO - Scraping page 78...
[2025-03-27T06:58:41.531+0000] {logging_mixin.py:190} INFO - Scraping page 79...
[2025-03-27T06:59:04.354+0000] {logging_mixin.py:190} INFO - Scraping page 80...
[2025-03-27T06:59:22.025+0000] {logging_mixin.py:190} INFO - Scraping page 81...
[2025-03-27T06:59:40.703+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19603-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19603-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68bb12d0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T06:59:49.851+0000] {logging_mixin.py:190} INFO - Scraping page 82...
[2025-03-27T07:00:07.696+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19588-12: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19588-12 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca6871e740>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:00:21.394+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19588-6: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19588-6 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68de7250>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:00:27.869+0000] {logging_mixin.py:190} INFO - Scraping page 83...
[2025-03-27T07:00:45.507+0000] {logging_mixin.py:190} INFO - Scraping page 84...
[2025-03-27T07:01:02.705+0000] {logging_mixin.py:190} INFO - Scraping page 85...
[2025-03-27T07:01:20.203+0000] {logging_mixin.py:190} INFO - Scraping page 86...
[2025-03-27T07:01:38.378+0000] {logging_mixin.py:190} INFO - Scraping page 87...
[2025-03-27T07:01:55.992+0000] {logging_mixin.py:190} INFO - Scraping page 88...
[2025-03-27T07:02:12.720+0000] {logging_mixin.py:190} INFO - Scraping page 89...
[2025-03-27T07:02:30.814+0000] {logging_mixin.py:190} INFO - Scraping page 90...
[2025-03-27T07:02:46.895+0000] {logging_mixin.py:190} INFO - Scraping page 91...
[2025-03-27T07:03:11.825+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19464-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19464-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68bb1030>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:03:16.452+0000] {logging_mixin.py:190} INFO - Scraping page 92...
[2025-03-27T07:03:34.912+0000] {logging_mixin.py:190} INFO - Scraping page 93...
[2025-03-27T07:03:52.942+0000] {logging_mixin.py:190} INFO - Scraping page 94...
[2025-03-27T07:04:14.820+0000] {logging_mixin.py:190} INFO - Scraping page 95...
[2025-03-27T07:04:32.509+0000] {logging_mixin.py:190} INFO - Scraping page 96...
[2025-03-27T07:04:57.073+0000] {logging_mixin.py:190} INFO - Scraping page 97...
[2025-03-27T07:05:16.203+0000] {logging_mixin.py:190} INFO - Scraping page 98...
[2025-03-27T07:05:34.394+0000] {logging_mixin.py:190} INFO - Scraping page 99...
[2025-03-27T07:05:51.681+0000] {logging_mixin.py:190} INFO - Scraping page 100...
[2025-03-27T07:06:08.338+0000] {logging_mixin.py:190} INFO - Scraping page 101...
[2025-03-27T07:06:27.700+0000] {logging_mixin.py:190} INFO - Scraping page 102...
[2025-03-27T07:06:47.403+0000] {logging_mixin.py:190} INFO - Scraping page 103...
[2025-03-27T07:07:06.284+0000] {logging_mixin.py:190} INFO - Scraping page 104...
[2025-03-27T07:07:23.423+0000] {logging_mixin.py:190} INFO - Scraping page 105...
[2025-03-27T07:07:40.209+0000] {logging_mixin.py:190} INFO - Scraping page 106...
[2025-03-27T07:07:59.218+0000] {logging_mixin.py:190} INFO - Scraping page 107...
[2025-03-27T07:08:15.472+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19217-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19217-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca63ee2ef0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:08:27.308+0000] {logging_mixin.py:190} INFO - Scraping page 108...
[2025-03-27T07:08:52.489+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19183-10: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19183-10 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca6871f1c0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:08:57.627+0000] {logging_mixin.py:190} INFO - Scraping page 109...
[2025-03-27T07:09:15.908+0000] {logging_mixin.py:190} INFO - Scraping page 110...
[2025-03-27T07:09:36.895+0000] {logging_mixin.py:190} INFO - Scraping page 111...
[2025-03-27T07:09:53.868+0000] {logging_mixin.py:190} INFO - Scraping page 112...
[2025-03-27T07:10:08.234+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19134-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19134-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68bb2b90>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:10:24.092+0000] {logging_mixin.py:190} INFO - Scraping page 113...
[2025-03-27T07:10:51.337+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/19107-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/19107-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68b8ec80>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:10:52.340+0000] {logging_mixin.py:190} INFO - Scraping page 114...
[2025-03-27T07:11:09.174+0000] {logging_mixin.py:190} INFO - Scraping page 115...
[2025-03-27T07:11:25.348+0000] {logging_mixin.py:190} INFO - Scraping page 116...
[2025-03-27T07:11:46.658+0000] {logging_mixin.py:190} INFO - Scraping page 117...
[2025-03-27T07:12:06.127+0000] {logging_mixin.py:190} INFO - Scraping page 118...
[2025-03-27T07:12:26.160+0000] {logging_mixin.py:190} INFO - Scraping page 119...
[2025-03-27T07:12:42.709+0000] {logging_mixin.py:190} INFO - Scraping page 120...
[2025-03-27T07:13:00.725+0000] {logging_mixin.py:190} INFO - Scraping page 121...
[2025-03-27T07:13:21.596+0000] {logging_mixin.py:190} INFO - Scraping page 122...
[2025-03-27T07:13:40.784+0000] {logging_mixin.py:190} INFO - Scraping page 123...
[2025-03-27T07:13:59.587+0000] {logging_mixin.py:190} INFO - Scraping page 124...
[2025-03-27T07:14:16.172+0000] {logging_mixin.py:190} INFO - Scraping page 125...
[2025-03-27T07:14:34.089+0000] {logging_mixin.py:190} INFO - Scraping page 126...
[2025-03-27T07:14:51.721+0000] {logging_mixin.py:190} INFO - Scraping page 127...
[2025-03-27T07:15:10.002+0000] {logging_mixin.py:190} INFO - Scraping page 128...
[2025-03-27T07:15:28.866+0000] {logging_mixin.py:190} INFO - Scraping page 129...
[2025-03-27T07:15:49.194+0000] {logging_mixin.py:190} INFO - Scraping page 130...
[2025-03-27T07:16:10.106+0000] {logging_mixin.py:190} INFO - Scraping page 131...
[2025-03-27T07:16:26.974+0000] {logging_mixin.py:190} INFO - Scraping page 132...
[2025-03-27T07:16:45.974+0000] {logging_mixin.py:190} INFO - Scraping page 133...
[2025-03-27T07:17:03.892+0000] {logging_mixin.py:190} INFO - Scraping page 134...
[2025-03-27T07:17:22.396+0000] {logging_mixin.py:190} INFO - Scraping page 135...
[2025-03-27T07:17:41.587+0000] {logging_mixin.py:190} INFO - Scraping page 136...
[2025-03-27T07:17:59.771+0000] {logging_mixin.py:190} INFO - Scraping page 137...
[2025-03-27T07:18:16.367+0000] {logging_mixin.py:190} INFO - Scraping page 138...
[2025-03-27T07:18:33.894+0000] {logging_mixin.py:190} INFO - Scraping page 139...
[2025-03-27T07:18:50.765+0000] {logging_mixin.py:190} INFO - Scraping page 140...
[2025-03-27T07:19:09.671+0000] {logging_mixin.py:190} INFO - Scraping page 141...
[2025-03-27T07:19:25.732+0000] {logging_mixin.py:190} INFO - Scraping page 142...
[2025-03-27T07:19:44.023+0000] {logging_mixin.py:190} INFO - Scraping page 143...
[2025-03-27T07:20:02.532+0000] {logging_mixin.py:190} INFO - Scraping page 144...
[2025-03-27T07:20:20.915+0000] {logging_mixin.py:190} INFO - Scraping page 145...
[2025-03-27T07:20:38.079+0000] {logging_mixin.py:190} INFO - Scraping page 146...
[2025-03-27T07:20:54.708+0000] {logging_mixin.py:190} INFO - Scraping page 147...
[2025-03-27T07:21:12.957+0000] {logging_mixin.py:190} INFO - Scraping page 148...
[2025-03-27T07:21:29.983+0000] {logging_mixin.py:190} INFO - Scraping page 149...
[2025-03-27T07:21:51.398+0000] {logging_mixin.py:190} INFO - Scraping page 150...
[2025-03-27T07:22:11.712+0000] {logging_mixin.py:190} INFO - Scraping page 151...
[2025-03-27T07:22:28.344+0000] {logging_mixin.py:190} INFO - Scraping page 152...
[2025-03-27T07:22:46.425+0000] {logging_mixin.py:190} INFO - Scraping page 153...
[2025-03-27T07:23:04.984+0000] {logging_mixin.py:190} INFO - Scraping page 154...
[2025-03-27T07:23:22.123+0000] {logging_mixin.py:190} INFO - Scraping page 155...
[2025-03-27T07:23:39.265+0000] {logging_mixin.py:190} INFO - Scraping page 156...
[2025-03-27T07:23:57.932+0000] {logging_mixin.py:190} INFO - Scraping page 157...
[2025-03-27T07:24:17.440+0000] {logging_mixin.py:190} INFO - Scraping page 158...
[2025-03-27T07:24:35.418+0000] {logging_mixin.py:190} INFO - Scraping page 159...
[2025-03-27T07:24:58.165+0000] {logging_mixin.py:190} INFO - Scraping page 160...
[2025-03-27T07:25:16.306+0000] {logging_mixin.py:190} INFO - Scraping page 161...
[2025-03-27T07:25:34.923+0000] {logging_mixin.py:190} INFO - Scraping page 162...
[2025-03-27T07:25:53.540+0000] {logging_mixin.py:190} INFO - Scraping page 163...
[2025-03-27T07:26:11.861+0000] {logging_mixin.py:190} INFO - Scraping page 164...
[2025-03-27T07:26:30.986+0000] {logging_mixin.py:190} INFO - Scraping page 165...
[2025-03-27T07:26:49.883+0000] {logging_mixin.py:190} INFO - Scraping page 166...
[2025-03-27T07:27:08.642+0000] {logging_mixin.py:190} INFO - Scraping page 167...
[2025-03-27T07:27:26.399+0000] {logging_mixin.py:190} INFO - Scraping page 168...
[2025-03-27T07:27:49.509+0000] {logging_mixin.py:190} INFO - Scraping page 169...
[2025-03-27T07:28:07.782+0000] {logging_mixin.py:190} INFO - Scraping page 170...
[2025-03-27T07:28:26.123+0000] {logging_mixin.py:190} INFO - Scraping page 171...
[2025-03-27T07:28:50.616+0000] {logging_mixin.py:190} INFO - Scraping page 172...
[2025-03-27T07:29:07.815+0000] {logging_mixin.py:190} INFO - Scraping page 173...
[2025-03-27T07:29:25.744+0000] {logging_mixin.py:190} INFO - Scraping page 174...
[2025-03-27T07:29:43.162+0000] {logging_mixin.py:190} INFO - Scraping page 175...
[2025-03-27T07:30:04.663+0000] {logging_mixin.py:190} INFO - Scraping page 176...
[2025-03-27T07:30:21.201+0000] {logging_mixin.py:190} INFO - Scraping page 177...
[2025-03-27T07:30:38.058+0000] {logging_mixin.py:190} INFO - Scraping page 178...
[2025-03-27T07:30:55.444+0000] {logging_mixin.py:190} INFO - Scraping page 179...
[2025-03-27T07:31:12.353+0000] {logging_mixin.py:190} INFO - Scraping page 180...
[2025-03-27T07:31:29.416+0000] {logging_mixin.py:190} INFO - Scraping page 181...
[2025-03-27T07:31:45.964+0000] {logging_mixin.py:190} INFO - Scraping page 182...
[2025-03-27T07:32:07.311+0000] {logging_mixin.py:190} INFO - Scraping page 183...
[2025-03-27T07:32:23.450+0000] {logging_mixin.py:190} INFO - Scraping page 184...
[2025-03-27T07:32:43.931+0000] {logging_mixin.py:190} INFO - Scraping page 185...
[2025-03-27T07:33:03.866+0000] {logging_mixin.py:190} INFO - Scraping page 186...
[2025-03-27T07:33:20.942+0000] {logging_mixin.py:190} INFO - Scraping page 187...
[2025-03-27T07:33:36.856+0000] {logging_mixin.py:190} INFO - Scraping page 188...
[2025-03-27T07:33:53.192+0000] {logging_mixin.py:190} INFO - Scraping page 189...
[2025-03-27T07:34:13.586+0000] {logging_mixin.py:190} INFO - Scraping page 190...
[2025-03-27T07:34:31.189+0000] {logging_mixin.py:190} INFO - Scraping page 191...
[2025-03-27T07:34:48.434+0000] {logging_mixin.py:190} INFO - Scraping page 192...
[2025-03-27T07:35:06.541+0000] {logging_mixin.py:190} INFO - Scraping page 193...
[2025-03-27T07:35:22.518+0000] {logging_mixin.py:190} INFO - Scraping page 194...
[2025-03-27T07:35:39.414+0000] {logging_mixin.py:190} INFO - Scraping page 195...
[2025-03-27T07:35:56.622+0000] {logging_mixin.py:190} INFO - Scraping page 196...
[2025-03-27T07:36:13.407+0000] {logging_mixin.py:190} INFO - Scraping page 197...
[2025-03-27T07:36:29.468+0000] {logging_mixin.py:190} INFO - Scraping page 198...
[2025-03-27T07:36:46.926+0000] {logging_mixin.py:190} INFO - Scraping page 199...
[2025-03-27T07:37:05.485+0000] {logging_mixin.py:190} INFO - Scraping page 200...
[2025-03-27T07:37:23.370+0000] {logging_mixin.py:190} INFO - Scraping page 201...
[2025-03-27T07:37:42.794+0000] {logging_mixin.py:190} INFO - Scraping page 202...
[2025-03-27T07:38:03.976+0000] {logging_mixin.py:190} INFO - Scraping page 203...
[2025-03-27T07:38:23.564+0000] {logging_mixin.py:190} INFO - Scraping page 204...
[2025-03-27T07:38:40.287+0000] {logging_mixin.py:190} INFO - Scraping page 205...
[2025-03-27T07:39:00.939+0000] {logging_mixin.py:190} INFO - Scraping page 206...
[2025-03-27T07:39:17.455+0000] {logging_mixin.py:190} INFO - Scraping page 207...
[2025-03-27T07:39:34.871+0000] {logging_mixin.py:190} INFO - Scraping page 208...
[2025-03-27T07:39:53.198+0000] {logging_mixin.py:190} INFO - Scraping page 209...
[2025-03-27T07:40:10.005+0000] {logging_mixin.py:190} INFO - Scraping page 210...
[2025-03-27T07:40:28.051+0000] {logging_mixin.py:190} INFO - Scraping page 211...
[2025-03-27T07:40:46.512+0000] {logging_mixin.py:190} INFO - Scraping page 212...
[2025-03-27T07:41:04.063+0000] {logging_mixin.py:190} INFO - Scraping page 213...
[2025-03-27T07:41:24.086+0000] {logging_mixin.py:190} INFO - Scraping page 214...
[2025-03-27T07:41:41.700+0000] {logging_mixin.py:190} INFO - Scraping page 215...
[2025-03-27T07:41:58.846+0000] {logging_mixin.py:190} INFO - Scraping page 216...
[2025-03-27T07:42:16.882+0000] {logging_mixin.py:190} INFO - Scraping page 217...
[2025-03-27T07:42:35.163+0000] {logging_mixin.py:190} INFO - Scraping page 218...
[2025-03-27T07:42:55.995+0000] {logging_mixin.py:190} INFO - Scraping page 219...
[2025-03-27T07:43:13.672+0000] {logging_mixin.py:190} INFO - Scraping page 220...
[2025-03-27T07:43:32.633+0000] {logging_mixin.py:190} INFO - Scraping page 221...
[2025-03-27T07:43:50.354+0000] {logging_mixin.py:190} INFO - Scraping page 222...
[2025-03-27T07:44:07.930+0000] {logging_mixin.py:190} INFO - Scraping page 223...
[2025-03-27T07:44:26.838+0000] {logging_mixin.py:190} INFO - Scraping page 224...
[2025-03-27T07:44:43.268+0000] {logging_mixin.py:190} INFO - Scraping page 225...
[2025-03-27T07:45:00.519+0000] {logging_mixin.py:190} INFO - Scraping page 226...
[2025-03-27T07:45:17.262+0000] {logging_mixin.py:190} INFO - Scraping page 227...
[2025-03-27T07:45:34.492+0000] {logging_mixin.py:190} INFO - Scraping page 228...
[2025-03-27T07:45:52.686+0000] {logging_mixin.py:190} INFO - Scraping page 229...
[2025-03-27T07:46:14.101+0000] {logging_mixin.py:190} INFO - Scraping page 230...
[2025-03-27T07:46:33.020+0000] {logging_mixin.py:190} INFO - Scraping page 231...
[2025-03-27T07:46:51.042+0000] {logging_mixin.py:190} INFO - Scraping page 232...
[2025-03-27T07:47:07.297+0000] {logging_mixin.py:190} INFO - Scraping page 233...
[2025-03-27T07:47:25.199+0000] {logging_mixin.py:190} INFO - Scraping page 234...
[2025-03-27T07:47:42.247+0000] {logging_mixin.py:190} INFO - Scraping page 235...
[2025-03-27T07:48:05.090+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/12409-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/12409-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68532bc0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:48:09.821+0000] {logging_mixin.py:190} INFO - Scraping page 236...
[2025-03-27T07:48:29.393+0000] {logging_mixin.py:190} INFO - Scraping page 237...
[2025-03-27T07:48:46.123+0000] {logging_mixin.py:190} INFO - Scraping page 238...
[2025-03-27T07:49:03.792+0000] {logging_mixin.py:190} INFO - Scraping page 239...
[2025-03-27T07:49:22.446+0000] {logging_mixin.py:190} INFO - Scraping page 240...
[2025-03-27T07:49:39.395+0000] {logging_mixin.py:190} INFO - Scraping page 241...
[2025-03-27T07:49:57.364+0000] {logging_mixin.py:190} INFO - Scraping page 242...
[2025-03-27T07:50:15.120+0000] {logging_mixin.py:190} INFO - Scraping page 243...
[2025-03-27T07:50:32.831+0000] {logging_mixin.py:190} INFO - Scraping page 244...
[2025-03-27T07:50:51.212+0000] {logging_mixin.py:190} INFO - Scraping page 245...
[2025-03-27T07:51:09.532+0000] {logging_mixin.py:190} INFO - Scraping page 246...
[2025-03-27T07:51:26.728+0000] {logging_mixin.py:190} INFO - Scraping page 247...
[2025-03-27T07:51:44.708+0000] {logging_mixin.py:190} INFO - Scraping page 248...
[2025-03-27T07:52:01.297+0000] {logging_mixin.py:190} INFO - Scraping page 249...
[2025-03-27T07:52:16.811+0000] {logging_mixin.py:190} INFO - Scraping page 250...
[2025-03-27T07:52:32.191+0000] {logging_mixin.py:190} INFO - Scraping page 251...
[2025-03-27T07:52:47.534+0000] {logging_mixin.py:190} INFO - Scraping page 252...
[2025-03-27T07:53:07.082+0000] {logging_mixin.py:190} INFO - Scraping page 253...
[2025-03-27T07:53:22.543+0000] {logging_mixin.py:190} INFO - Scraping page 254...
[2025-03-27T07:53:39.191+0000] {logging_mixin.py:190} INFO - Scraping page 255...
[2025-03-27T07:53:56.971+0000] {logging_mixin.py:190} INFO - Scraping page 256...
[2025-03-27T07:54:12.031+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/12185-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/12185-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca68b8d180>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:54:22.138+0000] {logging_mixin.py:190} INFO - Scraping page 257...
[2025-03-27T07:54:45.294+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/12157-3: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/12157-3 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca684eb7c0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T07:54:47.145+0000] {logging_mixin.py:190} INFO - Scraping page 258...
[2025-03-27T07:55:04.616+0000] {logging_mixin.py:190} INFO - Scraping page 259...
[2025-03-27T07:55:21.231+0000] {logging_mixin.py:190} INFO - Scraping page 260...
[2025-03-27T07:55:36.836+0000] {logging_mixin.py:190} INFO - Scraping page 261...
[2025-03-27T07:55:53.778+0000] {logging_mixin.py:190} INFO - Scraping page 262...
[2025-03-27T07:56:10.163+0000] {logging_mixin.py:190} INFO - Scraping page 263...
[2025-03-27T07:56:26.603+0000] {logging_mixin.py:190} INFO - Scraping page 264...
[2025-03-27T07:56:42.974+0000] {logging_mixin.py:190} INFO - Scraping page 265...
[2025-03-27T07:57:00.999+0000] {logging_mixin.py:190} INFO - Scraping page 266...
[2025-03-27T07:57:18.020+0000] {logging_mixin.py:190} INFO - Scraping page 267...
[2025-03-27T07:57:33.686+0000] {logging_mixin.py:190} INFO - Scraping page 268...
[2025-03-27T07:57:50.287+0000] {logging_mixin.py:190} INFO - Scraping page 269...
[2025-03-27T07:58:06.626+0000] {logging_mixin.py:190} INFO - Scraping page 270...
[2025-03-27T07:58:22.921+0000] {logging_mixin.py:190} INFO - Scraping page 271...
[2025-03-27T07:58:39.084+0000] {logging_mixin.py:190} INFO - Scraping page 272...
[2025-03-27T07:58:56.039+0000] {logging_mixin.py:190} INFO - Scraping page 273...
[2025-03-27T07:59:12.675+0000] {logging_mixin.py:190} INFO - Scraping page 274...
[2025-03-27T07:59:28.778+0000] {logging_mixin.py:190} INFO - Scraping page 275...
[2025-03-27T07:59:45.056+0000] {logging_mixin.py:190} INFO - Scraping page 276...
[2025-03-27T08:00:01.178+0000] {logging_mixin.py:190} INFO - Scraping page 277...
[2025-03-27T08:00:23.617+0000] {logging_mixin.py:190} INFO - Scraping page 278...
[2025-03-27T08:00:41.111+0000] {logging_mixin.py:190} INFO - Scraping page 279...
[2025-03-27T08:00:57.037+0000] {logging_mixin.py:190} INFO - Scraping page 280...
[2025-03-27T08:01:13.650+0000] {logging_mixin.py:190} INFO - Scraping page 281...
[2025-03-27T08:01:29.381+0000] {logging_mixin.py:190} INFO - Scraping page 282...
[2025-03-27T08:01:49.780+0000] {logging_mixin.py:190} INFO - Scraping page 283...
[2025-03-27T08:02:07.077+0000] {logging_mixin.py:190} INFO - Scraping page 284...
[2025-03-27T08:02:23.360+0000] {logging_mixin.py:190} INFO - Scraping page 285...
[2025-03-27T08:02:39.991+0000] {logging_mixin.py:190} INFO - Scraping page 286...
[2025-03-27T08:02:58.580+0000] {logging_mixin.py:190} INFO - Scraping page 287...
[2025-03-27T08:03:20.555+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/11756-1: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/11756-1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca683dc9a0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T08:03:27.416+0000] {logging_mixin.py:190} INFO - Scraping page 288...
[2025-03-27T08:03:50.432+0000] {logging_mixin.py:190} INFO - Scraping page 289...
[2025-03-27T08:04:09.178+0000] {logging_mixin.py:190} INFO - Scraping page 290...
[2025-03-27T08:04:30.846+0000] {logging_mixin.py:190} INFO - Scraping page 291...
[2025-03-27T08:04:50.785+0000] {logging_mixin.py:190} INFO - Scraping page 292...
[2025-03-27T08:05:08.539+0000] {logging_mixin.py:190} INFO - Scraping page 293...
[2025-03-27T08:05:24.778+0000] {logging_mixin.py:190} INFO - Scraping page 294...
[2025-03-27T08:05:42.391+0000] {logging_mixin.py:190} INFO - Scraping page 295...
[2025-03-27T08:06:00.329+0000] {logging_mixin.py:190} INFO - Scraping page 296...
[2025-03-27T08:06:19.227+0000] {logging_mixin.py:190} INFO - Scraping page 297...
[2025-03-27T08:06:35.206+0000] {logging_mixin.py:190} INFO - Scraping page 298...
[2025-03-27T08:06:52.145+0000] {logging_mixin.py:190} INFO - Scraping page 299...
[2025-03-27T08:07:11.284+0000] {logging_mixin.py:190} INFO - Scraping page 300...
[2025-03-27T08:07:36.395+0000] {logging_mixin.py:190} INFO - Error scraping https://teduh.kpkt.gov.my/project-swasta/11590-5: HTTPSConnectionPool(host='teduh.kpkt.gov.my', port=443): Max retries exceeded with url: /project-swasta/11590-5 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fca688225f0>, 'Connection to teduh.kpkt.gov.my timed out. (connect timeout=10)'))
[2025-03-27T08:07:38.994+0000] {logging_mixin.py:190} INFO - Scraping page 301...
[2025-03-27T08:07:55.798+0000] {logging_mixin.py:190} INFO - Scraping page 302...
[2025-03-27T08:08:13.100+0000] {logging_mixin.py:190} INFO - Scraping page 303...
[2025-03-27T08:08:31.074+0000] {logging_mixin.py:190} INFO - Scraping page 304...
[2025-03-27T08:08:51.031+0000] {logging_mixin.py:190} INFO - Scraping page 305...
[2025-03-27T08:09:08.426+0000] {logging_mixin.py:190} INFO - Scraping page 306...
[2025-03-27T08:09:25.653+0000] {logging_mixin.py:190} INFO - Scraping page 307...
[2025-03-27T08:09:43.082+0000] {logging_mixin.py:190} INFO - Scraping page 308...
[2025-03-27T08:10:00.427+0000] {logging_mixin.py:190} INFO - Scraping page 309...
[2025-03-27T08:52:01.833+0000] {job.py:229} INFO - Heartbeat recovered after 2510.70 seconds
[2025-03-27T08:52:01.864+0000] {local_task_job_runner.py:228} ERROR - Heartbeat time limit exceeded!
[2025-03-27T08:52:01.866+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-27T08:52:01.872+0000] {process_utils.py:132} INFO - Sending 15 to group 3945. PIDs of all processes in the group: [3945]
[2025-03-27T08:52:01.875+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 3945
[2025-03-27T08:52:02.804+0000] {taskinstance.py:3094} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-03-27T08:52:02.866+0000] {taskinstance.py:3095} ERROR - Stacktrace: 
  File "/home/***/.local/bin/***", line 8, in <module>
    sys.exit(main())
  File "/home/***/.local/lib/python3.10/site-packages/***/__main__.py", line 62, in main
    args.func(args)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 56, in scheduler
    run_command_with_daemon_option(
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 59, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/scheduler_job_runner.py", line 990, in _execute
    executor.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 392, in start
    self.impl.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 330, in start
    worker.start()
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 80, in run
    return super().run()
  File "/usr/local/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 211, in do_work
    self.execute_work(key=key, command=command)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 100, in execute_work
    state = self._execute_work_in_fork(command)
  File "/home/***/.local/lib/python3.10/site-packages/***/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/executors/local_executor.py", line 142, in _execute_work_in_fork
    args.func(args)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 254, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 322, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/jobs/local_task_job_runner.py", line 171, in _execute
    self.task_runner.start()
  File "/home/***/.local/lib/python3.10/site-packages/***/task/task_runner/standard_task_runner.py", line 55, in start
    self.process = self._start_by_fork()
  File "/home/***/.local/lib/python3.10/site-packages/***/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/***/.local/lib/python3.10/site-packages/***/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/***/.local/lib/python3.10/site-packages/***/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/***/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/***/dags/construction_web_scraping.py", line 183, in run_scraper
    daerah, negeri, harga_min, harga_maks = scrape_additional_data(link)
  File "/opt/***/dags/construction_web_scraping.py", line 45, in scrape_additional_data
    response = requests.get(link, timeout=10)
  File "/home/***/.local/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/***/.local/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "/home/***/.local/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/***/.local/lib/python3.10/site-packages/***/models/taskinstance.py", line 3095, in signal_handler
    self.log.error("Stacktrace: \n%s", "".join(traceback.format_stack()))

[2025-03-27T08:52:02.895+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/util/connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/local/lib/python3.10/socket.py", line 967, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/construction_web_scraping.py", line 183, in run_scraper
    daerah, negeri, harga_min, harga_maks = scrape_additional_data(link)
  File "/opt/airflow/dags/construction_web_scraping.py", line 45, in scrape_additional_data
    response = requests.get(link, timeout=10)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "/home/airflow/.local/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3097, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2025-03-27T08:52:02.910+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=construction_web_scraping_v01, task_id=scrape_and_store, run_id=scheduled__2025-03-09T00:00:00+00:00, execution_date=20250309T000000, start_date=20250327T063311, end_date=20250327T085202
[2025-03-27T08:52:02.988+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T08:52:03.373+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=3945, status='terminated', exitcode=2, started='06:33:11') (3945) terminated with exit code 2
